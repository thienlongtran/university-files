{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3220: Python for Data Science and AI\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "## Due Date: April 11(Monday), 11:59 PM\n",
    "\n",
    "## Total Points: 35\n",
    "\n",
    "## Bonus Points: 5\n",
    "\n",
    "#### ***Write the code in Jupyter notebook (ipynb file) with proper comments. \n",
    "#### ***Add proper citation if you take help from a different source (not from the textbook).\n",
    "#### ***Rename the file with your student ID and submit it in Moodle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import math\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a.\tLoad any one of the toy datasets (for classification) from scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Display general information and statistics about your selected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tSplit the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.\tImplement a k-nearest neighbors classifier (k =3) for the training set. Show the result for the test set using the following metric: accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Results:\n",
      " [0 0 1 0 0 2 0 1 0 2 2 2 0 1 2 2 2 0 2 2 1 1 0 1 0 2 0 0 2 0 1 0 2 0 1 2 1\n",
      " 1]\n",
      "True Results:\n",
      " [0 0 1 0 0 1 0 1 0 2 2 2 0 1 2 2 2 0 2 2 1 1 0 1 0 2 0 0 2 0 2 0 2 0 1 2 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "#Gets Distance Between Two Observations\n",
    "def getDistance(point1, point2):    \n",
    "    distance_sepal_length = ((point1[0] - point2[0]) ** 2)\n",
    "    distance_sepal_width = ((point1[1] - point2[1]) ** 2)\n",
    "    distance_petal_length = ((point1[2] - point2[2]) ** 2)\n",
    "    distance_petal_width = ((point1[3] - point2[3]) ** 2)\n",
    "    distance = math.sqrt(distance_sepal_length + distance_sepal_width + distance_petal_length + distance_petal_width) \n",
    "    return distance\n",
    "\n",
    "#Determines Species by KNN\n",
    "def determineSpecies(point, x_train, y_train, n):\n",
    "    distanceTrainObservationPair = dict()\n",
    "    \n",
    "    #Add Distance to Dictionary Observations\n",
    "    for train_observation in range(len(x_train)):\n",
    "        distanceTrainObservationPair.update({getDistance(point, x_train[train_observation]) : train_observation})\n",
    "        \n",
    "    #Citation: Learned How to Sort Dictionary on StackOverflow\n",
    "    #Link: https://stackoverflow.com/questions/9001509/how-can-i-sort-a-dictionary-by-key\n",
    "    distanceTrainObservationPair = collections.OrderedDict(sorted(distanceTrainObservationPair.items()))\n",
    "    distanceTrainObservationPair = dict(distanceTrainObservationPair)\n",
    "    \n",
    "    #Increments Index of Species Array to Lowest Three Observation's Label\n",
    "    species = [0, 0, 0]\n",
    "    for i in range(n):\n",
    "        train_observation_index = list(distanceTrainObservationPair.values())[i]\n",
    "        species[y_train[train_observation_index]] = species[y_train[train_observation_index]] + 1\n",
    "     \n",
    "    #Determine Which Species is Nearest Neighbors\n",
    "    current_largest_n_species = -1 \n",
    "    for i in range(len(species)):  \n",
    "        if(species[i] > current_largest_n_species):\n",
    "            current_largest_n_species = i\n",
    "    \n",
    "    return current_largest_n_species\n",
    "\n",
    "\n",
    "#Guess Species for Each Observation in x_test and Save to y_result\n",
    "y_predictions = np.arange(len(x_test)) #Determined Results by KNN\n",
    "for i in range(len(x_test)):\n",
    "    y_predictions[i] = determineSpecies(x_test[i], x_train, y_train, 3)\n",
    "\n",
    "print(\"Predicted Results:\\n\", y_predictions)\n",
    "print(\"True Results:\\n\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.74%\n"
     ]
    }
   ],
   "source": [
    "#Calculate Accuracy\n",
    "def getAccuracy(y_predictions, y_test):\n",
    "    correct_number_of_predictions = 0\n",
    "    for i in range(len(y_predictions)):\n",
    "        if(y_predictions[i] == y_test[i]):\n",
    "            correct_number_of_predictions = correct_number_of_predictions + 1\n",
    "    return (correct_number_of_predictions/len(y_predictions))*100\n",
    "\n",
    "def getConfusionMatrix(y_predictions, y_test):\n",
    "    num_true_positives = 0\n",
    "    num_true_negatives = 0\n",
    "    num_false_positives = 0\n",
    "    num_false_negatives = 0\n",
    "    \n",
    "    for i in range(len(y_predictions)):\n",
    "        pass\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" %(getAccuracy(y_predictions, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  0,  0],\n",
       "       [ 0,  9,  1],\n",
       "       [ 0,  1, 12]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.\tSplit the training set into 10-fold for cross-validation. Calculate accuracy for each held-out set. Also, display the average accuracy for ten folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(y_train)\n",
    "\n",
    "x_fold = list()\n",
    "y_fold = list()\n",
    "\n",
    "#Create Folds of Xs and Ys of Length 10\n",
    "for i in range(0, len(x_train), 10):\n",
    "    x_subset = x_train[i:i+10]\n",
    "    x_fold.append(x_subset)\n",
    "    y_subset = y_train[i:i+10]\n",
    "    y_fold.append(y_subset)\n",
    "\n",
    "#Train and Test using KFold\n",
    "for i in range(len(y_fold)-1):\n",
    "    #Guess Species for Each Observation in x_test and Save to y_result\n",
    "    y_predictions = np.arange(len(x_test)) #Determined Results by KNN\n",
    "    test_x = x_fold[i]\n",
    "    test_y = y_fold[i]\n",
    "    \n",
    "    for j in range(len(x_test)):\n",
    "        if j == i:\n",
    "            test_x = x_fold[j]\n",
    "            test_y = y_fold[j]\n",
    "        else:\n",
    "            y_predictions[j] = determineSpecies(x_test[i], x_train, y_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tFind the optimal value of k ranging from 1 to 10 based on mean accuracy of 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement the steps (3-5) with the scikit-learn library method and compare it with your previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Results:\n",
      " [1 1 1 0 1 0 1 2 1 2 2 2 2 0 1 2 1 0 1 0 2 0 1 2 0 2 1 0 2 2 0 1 2 2 1 0 0\n",
      " 1]\n",
      "True Results:\n",
      " [1 1 1 0 1 0 1 2 1 2 2 2 2 0 1 2 1 0 1 0 2 0 1 2 0 2 1 0 2 2 0 1 2 2 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris6 = load_iris()\n",
    "\n",
    "x_train6, x_test6, y_train6, y_test6 = train_test_split(iris6.data, iris6.target)\n",
    "\n",
    "KNearestNeighbors = KNeighborsClassifier(n_neighbors = 1)\n",
    "KNearestNeighbors.fit(x_train6, y_train6)\n",
    "\n",
    "y_predictions6 = KNearestNeighbors.predict(x_test6)\n",
    "\n",
    "print(\"Predicted Results:\\n\", y_predictions6)\n",
    "print(\"True Results:\\n\", y_test6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n",
      "Precision: 1\n",
      "Recall: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" %(accuracy_score(y_test6, y_predictions6) * 100))\n",
    "print(\"Precision: %d\" %(precision_score(y_test6, y_predictions6, average=\"macro\")))\n",
    "print(\"Recall: %d\" %(recall_score(y_test6, y_predictions6, average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Timeforcast.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
